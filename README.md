# <p align=center> LM Pretraining with Pytorch/TPU</p>
This repo makes it easy to train language models on PyTorch/TPU. It relies on two libraries, [PyTorch/XLA](https://github.com/pytorch/xla/) to run PyTorch code on TPUs, and [pytorch-transformers](https://github.com/huggingface/pytorch-transformers) for the language models implementation.

## How to use

###  Create Cloud TPU

To use TPUs, all your computations happen on Google Cloud. Use the command `ctpu` to instantiate a TPU and VM,

```
ctpu up -tf-version=pytorch-nightly -name [lm_tpu] -tpu-size [v3-8] -machine-type=[n1-standard-16] -zone=[us-central1-a] -gcp-network=[default] -project=[my_proj] [-preemptible] [-tpu-only]
```

-  Replace the parameters in square prackets with the right values for you. Make sure to get the `zone`, `gcp-network`, `preemptible`, `project` right, especially if you are using credit from TFRC.

- This command will create a Cloud TPU and a separate VM, and you need both to run your code. A `n1-standard-16` machine is reasonable sized as smaller machines might not be fast enough. If you already have a VM, add the argument `-tpu_only`.

- The `-tf-version=pytorch-nightly` argument is very important. It specifies that this TPU will be used to run PyTorch code (not Tensorflow code). It uses the nightly build, which has many bug fixes that are not in the prerelease `pytorch-0.1`.

- Our code only supports Cloud TPUs (v2-8 and v3-8), and not the larger TPU pods. We will add support for those in the future.

- It is easier to use the `ctpu` command than using the Google Cloud console interface. `ctpu` automatically finds an IP for the TPU, and sets the right permissions for the VM


###  Setup environemnt

- ssh to the VM created in the previous step

- Build docker image
```
git clone https://github.com/allenai/tpu_pretrain.git
cd tpu_pretrain
docker build . -t tpu_pretrain
```
While the environment can be setup in multiple different ways, we found that using docker is the most convenient way. Our docker image contains both libraries(PyTorch/XLA and pytorch-transformers). We use the "nighlty" build of PyTorch/XLA because it has many bug fixes that are important for our code to run, and that are not in the stable prerelease. 

- Run the docker container
```
docker run -it --mount type=bind,source="$(pwd)",target=/code  -e TPU_IP=[TPU_IP] tpu_pretrain
```
and replace `[TPU_IP]` with IP of the TPU you just created.

- To test that everything is working fine, run the mnist example
```
python /pytorch/xla/test/test_train_mnist.py
```

###  Run LM pretraining

```
cd /code
python -m pretrain  --pregenerated_data data/pregenerated_training_data/  --output_dir finetuned_roberta_base  --epochs 4  --bert_model  roberta-base  --tpu_ip $TPU_IP  --train_batch_size 24
```
It fine tunes the roberta-base model on the sample pregenerated training data on `data/pregenerated_training_data/`. Each epoch will take around 15 minutes. Notice that the first few steps are usually slower than the rest because the TPU compiles the graph in the first steps, then use the cached compiled one for subsequent steps.


###  Pregenerate training data

The pretraining code assumes pregenerated training data, which is generated by the script `pytorch_transformers_lm_finetuning/pregenerate_training_data.py`. This script is adopted from the one on [pytorch-transformers](https://github.com/huggingface/pytorch-transformers/blob/master/examples/lm_finetuning/pregenerate_training_data.py) with some modefications. It takes as input raw text and outputs the format needed for the pretraining script. The input format
is a glob of text files, each one has one sentence per line, and an empty line as document separator.

```
python  pytorch_transformers_lm_finetuning/pregenerate_training_data.py  --train_corpus  "data/sentences_150k.txt"  --output data/pregenerated_training_data --bert_model roberta-base  --do_whole_word_mask  --epochs_to_generate 2  --max_seq_len 512  --max_predictions_per_seq 75
```

- If your corpus is one large file, please split it into smaller files before generating the training data, each is not more than 500K sentences.

- If you have large number of files, consider using the argument `--num_workers x`.


###  Debugging and common issues

Slow first few steps, OOM, --one_tpu, restart tpu, .item(), metric report, 


###  Runing on TPU Pods (large TPUs)

PyTorch/XLA TPU pod training is not working yet. 
